{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de bibliotecas necesarias if you want to download the dataset from kaggle\n",
        "#import kagglehub\n",
        "#import shutil\n",
        "\n",
        "# Descarga del conjunto de datos de Kaggle\n",
        "# Este conjunto contiene im√°genes de tomograf√≠as computarizadas del cerebro\n",
        "# con casos de ictus y casos normales\n",
        "#path = kagglehub.dataset_download(\"afridirahman/brain-stroke-ct-image-dataset\")\n",
        "#print(\"Ruta a los archivos del dataset:\", path)\n",
        "\n",
        "# Copia de los datos descargados a una ubicaci√≥n local\n",
        "# Esto nos permite trabajar con los datos sin necesidad de volver a descargarlos\n",
        "#src = path  # Usamos la variable path que contiene la ubicaci√≥n de descarga\n",
        "#dst = r'.\\Dataset_kaggle'\n",
        "\n",
        "#shutil.copytree(src, dst)\n",
        "#print(f\"Dataset copiado de {src} a {dst}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Creando directorios de destino...\n",
            "\n",
            "üîÑ Procesando clase: Stroke\n",
            "   Buscando archivos en: ../../data/Brain_Data_Organised\\Stroke\n",
            "   Im√°genes encontradas: 1900\n",
            "   Divisi√≥n: 1520 para entrenamiento, 380 para prueba\n",
            "   ‚úÖ Primer archivo de entrenamiento: 67 (14).jpg\n",
            "   ‚úÖ Primer archivo de prueba: 75 (24).jpg\n",
            "\n",
            "üîÑ Procesando clase: Normal\n",
            "   Buscando archivos en: ../../data/Brain_Data_Organised\\Normal\n",
            "   Im√°genes encontradas: 2338\n",
            "   Divisi√≥n: 1870 para entrenamiento, 468 para prueba\n",
            "   ‚úÖ Primer archivo de entrenamiento: 54 (7).jpg\n",
            "   ‚úÖ Primer archivo de prueba: 51 (16).jpg\n",
            "\n",
            "==================================================\n",
            "üìä Estad√≠sticas finales:\n",
            "   train/Stroke: 913 archivos\n",
            "   train/Normal: 1122 archivos\n",
            "   test/Stroke: 343 archivos\n",
            "   test/Normal: 421 archivos\n",
            "\n",
            "‚úÖ ¬°Distribuci√≥n completada!\n"
          ]
        }
      ],
      "source": [
        "# Importaci√≥n de bibliotecas necesarias para el procesamiento de datos\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuraci√≥n de directorios y par√°metros\n",
        "source_dir = '../../data/Brain_Data_Organised'  # Directorio fuente con las im√°genes originales\n",
        "target_dir = 'Dataset_img_for_CNN'  # Directorio donde organizaremos los datos para el entrenamiento\n",
        "split_ratio = 0.8  # Proporci√≥n 80% para entrenamiento, 20% para prueba\n",
        "classes = ['Stroke', 'Normal']  # Las dos clases que queremos clasificar\n",
        "\n",
        "# Creaci√≥n de la estructura de directorios para el entrenamiento\n",
        "print(\"üìÅ Creando directorios de destino...\")\n",
        "for phase in ['train', 'test']:\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(target_dir, phase, cls), exist_ok=True)\n",
        "\n",
        "# Configuraci√≥n de la semilla aleatoria para reproducibilidad\n",
        "random.seed(42)  # Esto asegura que obtengamos los mismos resultados cada vez que ejecutemos el c√≥digo\n",
        "\n",
        "# Procesamiento de cada clase\n",
        "for cls in classes:\n",
        "    cls_dir = os.path.join(source_dir, cls)\n",
        "    \n",
        "    print(f\"\\nüîÑ Procesando clase: {cls}\")\n",
        "    print(f\"   Buscando archivos en: {cls_dir}\")\n",
        "    \n",
        "    # Verificaci√≥n de la existencia del directorio\n",
        "    if not os.path.exists(cls_dir):\n",
        "        print(f\"‚ùå ¬°Carpeta {cls_dir} no encontrada!\")\n",
        "        continue\n",
        "    \n",
        "    # B√∫squeda de im√°genes en formatos comunes\n",
        "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif']\n",
        "    images = []\n",
        "    \n",
        "    # Recopilaci√≥n de todas las im√°genes disponibles\n",
        "    for ext in image_extensions:\n",
        "        images.extend(list(Path(cls_dir).glob(ext)))\n",
        "        images.extend(list(Path(cls_dir).glob(ext.upper())))  # Tambi√©n en may√∫sculas\n",
        "    \n",
        "    print(f\"   Im√°genes encontradas: {len(images)}\")\n",
        "    \n",
        "    if len(images) == 0:\n",
        "        print(f\"‚ö†Ô∏è  No hay im√°genes en la carpeta {cls_dir}\")\n",
        "        continue\n",
        "    \n",
        "    # Divisi√≥n aleatoria de las im√°genes\n",
        "    random.shuffle(images)\n",
        "    split_idx = int(len(images) * split_ratio)\n",
        "    train_images = images[:split_idx]\n",
        "    test_images = images[split_idx:]\n",
        "    \n",
        "    print(f\"   Divisi√≥n: {len(train_images)} para entrenamiento, {len(test_images)} para prueba\")\n",
        "    \n",
        "    # Copia de archivos a sus respectivos directorios\n",
        "    for i, img_path in enumerate(train_images):\n",
        "        try:\n",
        "            dest_path = os.path.join(target_dir, 'train', cls, img_path.name)\n",
        "            shutil.copy2(img_path, dest_path)\n",
        "            if i == 0:  # Imprimir el primer archivo para verificaci√≥n\n",
        "                print(f\"   ‚úÖ Primer archivo de entrenamiento: {img_path.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error al copiar {img_path.name}: {e}\")\n",
        "    \n",
        "    for i, img_path in enumerate(test_images):\n",
        "        try:\n",
        "            dest_path = os.path.join(target_dir, 'test', cls, img_path.name)\n",
        "            shutil.copy2(img_path, dest_path)\n",
        "            if i == 0:  # Imprimir el primer archivo para verificaci√≥n\n",
        "                print(f\"   ‚úÖ Primer archivo de prueba: {img_path.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error al copiar {img_path.name}: {e}\")\n",
        "\n",
        "# Verificaci√≥n de resultados\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä Estad√≠sticas finales:\")\n",
        "for phase in ['train', 'test']:\n",
        "    for cls in classes:\n",
        "        path = os.path.join(target_dir, phase, cls)\n",
        "        if os.path.exists(path):\n",
        "            count = len(os.listdir(path))\n",
        "            print(f\"   {phase}/{cls}: {count} archivos\")\n",
        "\n",
        "print(\"\\n‚úÖ ¬°Distribuci√≥n completada!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHAkgxSRAUl3"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. –êumentaci√≥n\n",
        "\n",
        "La aumentaci√≥n de datos es un proceso muy potente que permite aumentar la cantidad de datos de entrenamiento. Mediante rotaciones, reflejos, adici√≥n de ruido, desplazamientos y otras transformaciones, la imagen cambia ligeramente, pero mantiene su etiqueta original. Con la funci√≥n Compose podemos combinar varias transformaciones de imagen y luego aplicarlas al leer el conjunto de datos. La lista completa de aumentaciones est√° disponible [aqu√≠](https://pytorch.org/vision/stable/transforms.html). Est√∫diala y experimenta con diferentes transformaciones de imagen.\n",
        "\n",
        "Una herramienta bastante potente y eficiente para la aumentaci√≥n de im√°genes es la biblioteca `albumentations`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5S2icgGFIIZ",
        "outputId": "5c957221-75fc-4d6e-c53f-4dbdc58d9531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 1928\n",
            "Validation samples: 483\n",
            "Test samples: 911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Transformaciones para el conjunto de entrenamiento\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Redimensionar todas las im√°genes a 224x224 p√≠xeles\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convertir a escala de grises pero mantener 3 canales\n",
        "    transforms.RandomHorizontalFlip(),  # Volteo horizontal aleatorio para aumentar la variedad\n",
        "    transforms.RandomRotation(5),  # Rotaci√≥n aleatoria de hasta 5 grados\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),  # Variaci√≥n aleatoria de brillo y contraste\n",
        "    transforms.ToTensor(),  # Convertir imagen a tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalizaci√≥n de los valores de p√≠xeles\n",
        "])\n",
        "\n",
        "# Transformaciones para el conjunto de validaci√≥n y prueba\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Mismo tama√±o que el conjunto de entrenamiento\n",
        "    transforms.Grayscale(num_output_channels=3),  # Misma conversi√≥n a escala de grises\n",
        "    transforms.ToTensor(),  # Convertir a tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Misma normalizaci√≥n\n",
        "])\n",
        "\n",
        "# Load the full training dataset\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root='Dataset_img_for_CNN/train',\n",
        "    transform=train_transforms\n",
        ")\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    root='Dataset_img_for_CNN/test',\n",
        "    transform=val_test_transforms\n",
        ")\n",
        "\n",
        "# Split training data into train and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "# Use random_split to create train and validation datasets\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create validation dataset with appropriate transforms\n",
        "val_dataset.dataset = datasets.ImageFolder(\n",
        "    root='Dataset_img_for_CNN/train',\n",
        "    transform=val_test_transforms\n",
        ")\n",
        "val_dataset = torch.utils.data.Subset(val_dataset.dataset, val_dataset.indices)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3. Regularizaci√≥n y normalizaci√≥n en redes neuronales\n",
        "\n",
        "### Dropout\n",
        "Si la red tiene una arquitectura compleja, es posible el sobreajuste (overfitting) - un proceso en el que el modelo se adapta demasiado a los datos de entrenamiento y luego da un rendimiento inferior en los datos de prueba. Para combatir esto, se puede utilizar Dropout. La idea del m√©todo es muy simple. Durante el entrenamiento, `torch.nn.Dropout` establece a cero cada elemento del tensor de entrada con una probabilidad $p$. Durante la inferencia, no se establece nada a cero, pero para mantener la escala de las salidas de la red, todos los elementos del tensor de entrada se dividen por $1 - p$.\n",
        "\n",
        "![Dropout](https://github.com/hse-ds/iad-deep-learning/blob/master/2022/seminars/sem03/static/dropout.png?raw=1)\n",
        "\n",
        "Para estabilizar y acelerar la convergencia del entrenamiento, se utiliza frecuentemente la normalizaci√≥n por lotes (batch normalization). En **PyTorch** tambi√©n est√° implementada como una capa ‚Äî [`torch.nn.BatchNorm2d`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html). Generalmente, la normalizaci√≥n por lotes se inserta entre los bloques significativos de la red neuronal para mantener la distribuci√≥n de los datos durante todo el forward pass. Tenga en cuenta que durante el entrenamiento, la media y la desviaci√≥n est√°ndar muestral se calculan de nuevo para cada lote, y la capa tiene dos par√°metros num√©ricos entrenables para cada canal del tensor de entrada. Durante la inferencia, se utilizan como media y varianza las estimaciones obtenidas mediante promedios m√≥viles durante el entrenamiento.\n",
        "\n",
        "![Batch Norm](https://github.com/hse-ds/iad-deep-learning/blob/master/2022/seminars/sem03/static/batch_norm.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Typical CNN architecture](Typical%20CNN%20architecture.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu9h2Rb4Fg_K",
        "outputId": "f07c31a2-4c48-4e59-82b2-38b018a40790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Definici√≥n de la arquitectura CNN\n",
        "model = nn.Sequential(\n",
        "    # Primera capa convolucional\n",
        "    nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # 3 canales de entrada, 32 filtros\n",
        "    nn.BatchNorm2d(32),  # Normalizaci√≥n por lotes\n",
        "    nn.ReLU(),  # Funci√≥n de activaci√≥n\n",
        "    nn.MaxPool2d(2, 2),  # Reducci√≥n de dimensionalidad\n",
        "\n",
        "    # Segunda capa convolucional\n",
        "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 32 canales de entrada, 64 filtros\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "\n",
        "    # Tercera capa convolucional\n",
        "    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # 64 canales de entrada, 128 filtros\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "\n",
        "    # Capas fully connected\n",
        "    nn.Flatten(),  # Aplanar la salida para las capas densas\n",
        "    nn.Dropout(0.1),  # Regularizaci√≥n para evitar overfitting\n",
        "    nn.Linear(128*28*28, 512),  # Primera capa densa\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.Linear(512, 2)  # Capa de salida (2 clases)\n",
        ")\n",
        "\n",
        "# Set device and move model to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Class torch.nn.Conv2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "En **PyTorch**, la capa convolucional est√° representada en el m√≥dulo `torch.nn` por la clase [`Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) con los siguientes par√°metros:\n",
        "- `in_channels`: n√∫mero de canales de entrada\n",
        "- `out_channels`: n√∫mero de canales de salida\n",
        "- `kernel_size`: tama√±o del kernel (n√∫cleo)\n",
        "- `stride`: paso (desplazamiento)\n",
        "- `padding`: relleno\n",
        "- `padding_mode`: modo de relleno (`'zeros'`, `'reflect'` y otros)\n",
        "- `dilation`: dilataci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### `kernel_size`\n",
        "\n",
        "**Tama√±o del kernel (n√∫cleo)**. `int`, si el kernel es cuadrado, y una tupla de dos n√∫meros si el kernel es rectangular. Define el tama√±o del filtro con el que se realiza la convoluci√≥n de la imagen.\n",
        "\n",
        "**`kernel_size=3`**\n",
        "\n",
        "![no_padding_no_strides.gif](static/no_padding_no_strides.gif)\n",
        "\n",
        "Esta y las siguientes animaciones est√°n tomadas de [aqu√≠](https://github.com/vdumoulin/conv_arithmetic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### `stride`\n",
        "\n",
        "**Paso (stride)**. Define el paso, en p√≠xeles, con el que se desplaza el filtro. `int`, si el desplazamiento es el mismo en horizontal y vertical. Una tupla de dos n√∫meros, si los desplazamientos son diferentes.\n",
        "\n",
        "**`stride=2`**\n",
        "\n",
        "![no_padding_strides.gif](static/no_padding_strides.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### `padding`\n",
        "\n",
        "**Relleno (padding)**. Cantidad de p√≠xeles con los que se complementa la imagen. Similar al paso y al tama√±o del kernel, puede ser tanto `int` como una tupla de dos n√∫meros.\n",
        "\n",
        "**`padding=1`**\n",
        "\n",
        "![same_padding_no_strides.gif](static/same_padding_no_strides.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1XgxPUZFvcs",
        "outputId": "0ad768dc-f362-4243-8934-e58fc7f92960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "======================================================================\n",
            "Epoch [1/20]\n",
            "  Train Loss: 0.8707, Train Accuracy: 65.35%\n",
            "  Val Loss: 0.4867, Val Accuracy: 73.29%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [2/20]\n",
            "  Train Loss: 0.5155, Train Accuracy: 74.90%\n",
            "  Val Loss: 0.4168, Val Accuracy: 78.05%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [3/20]\n",
            "  Train Loss: 0.5135, Train Accuracy: 74.12%\n",
            "  Val Loss: 0.3637, Val Accuracy: 80.75%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [4/20]\n",
            "  Train Loss: 0.4448, Train Accuracy: 77.54%\n",
            "  Val Loss: 0.3550, Val Accuracy: 81.37%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [5/20]\n",
            "  Train Loss: 0.3351, Train Accuracy: 83.71%\n",
            "  Val Loss: 0.2728, Val Accuracy: 86.13%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [6/20]\n",
            "  Train Loss: 0.2782, Train Accuracy: 87.76%\n",
            "  Val Loss: 0.2445, Val Accuracy: 88.20%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [7/20]\n",
            "  Train Loss: 0.2424, Train Accuracy: 89.73%\n",
            "  Val Loss: 0.2211, Val Accuracy: 90.48%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [8/20]\n",
            "  Train Loss: 0.2808, Train Accuracy: 87.97%\n",
            "  Val Loss: 0.2056, Val Accuracy: 90.89%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [9/20]\n",
            "  Train Loss: 0.2385, Train Accuracy: 89.89%\n",
            "  Val Loss: 0.1846, Val Accuracy: 91.51%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [10/20]\n",
            "  Train Loss: 0.1878, Train Accuracy: 92.43%\n",
            "  Val Loss: 0.1991, Val Accuracy: 89.86%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [11/20]\n",
            "  Train Loss: 0.1544, Train Accuracy: 94.45%\n",
            "  Val Loss: 0.1536, Val Accuracy: 92.13%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [12/20]\n",
            "  Train Loss: 0.1332, Train Accuracy: 95.02%\n",
            "  Val Loss: 0.1488, Val Accuracy: 93.37%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [13/20]\n",
            "  Train Loss: 0.1345, Train Accuracy: 94.76%\n",
            "  Val Loss: 0.1177, Val Accuracy: 96.69%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [14/20]\n",
            "  Train Loss: 0.1029, Train Accuracy: 96.37%\n",
            "  Val Loss: 0.1313, Val Accuracy: 94.82%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [15/20]\n",
            "  Train Loss: 0.1198, Train Accuracy: 95.44%\n",
            "  Val Loss: 0.1542, Val Accuracy: 93.37%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [16/20]\n",
            "  Train Loss: 0.0954, Train Accuracy: 96.52%\n",
            "  Val Loss: 0.0970, Val Accuracy: 95.24%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [17/20]\n",
            "  Train Loss: 0.0860, Train Accuracy: 96.89%\n",
            "  Val Loss: 0.1134, Val Accuracy: 95.65%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [18/20]\n",
            "  Train Loss: 0.0758, Train Accuracy: 97.25%\n",
            "  Val Loss: 0.1616, Val Accuracy: 93.37%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [19/20]\n",
            "  Train Loss: 0.0670, Train Accuracy: 97.93%\n",
            "  Val Loss: 0.0854, Val Accuracy: 96.48%\n",
            "----------------------------------------------------------------------\n",
            "Epoch [20/20]\n",
            "  Train Loss: 0.0716, Train Accuracy: 97.72%\n",
            "  Val Loss: 0.0952, Val Accuracy: 96.07%\n",
            "----------------------------------------------------------------------\n",
            "Training completed!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Training parameters\n",
        "num_epochs = 20\n",
        "\n",
        "# Lists to store training history\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "\n",
        "    # Training batches\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        # Move data to device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate training metrics\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            # Move data to device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Statistics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f'  Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'  Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "    print('-' * 70)\n",
        "\n",
        "print('Training completed!')\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2ZrvuEUF2pk",
        "outputId": "5b8648c9-434a-46c9-ff37-d486d4ce5c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on test set...\n",
            "\n",
            "FINAL TEST RESULTS:\n",
            "Test Loss: 0.0618\n",
            "Test Accuracy: 98.13%\n",
            "======================================================================\n",
            "\n",
            "DETAILED CLASSIFICATION REPORT:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal     0.9946    0.9754    0.9849       568\n",
            "      stroke     0.9605    0.9913    0.9756       343\n",
            "\n",
            "    accuracy                         0.9813       911\n",
            "   macro avg     0.9775    0.9833    0.9802       911\n",
            "weighted avg     0.9818    0.9813    0.9814       911\n",
            "\n",
            "\n",
            "TRAINING SUMMARY:\n",
            "Best validation accuracy: 96.69% (Epoch 13)\n",
            "Final validation accuracy: 96.07%\n",
            "Final test accuracy: 98.13%\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Final evaluation on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "model.eval()\n",
        "\n",
        "# Collect all predictions and true labels for classification report\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move data to device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Statistics\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "        # Collect predictions and labels for classification report\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate final test metrics\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "\n",
        "print(f\"\\nFINAL TEST RESULTS:\")\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Generate classification report\n",
        "class_names = test_dataset.classes\n",
        "print(\"\\nDETAILED CLASSIFICATION REPORT:\")\n",
        "print(classification_report(all_labels, all_predictions,\n",
        "                          target_names=class_names,\n",
        "                          digits=4))\n",
        "\n",
        "# Print training summary\n",
        "print(\"\\nTRAINING SUMMARY:\")\n",
        "print(f\"Best validation accuracy: {max(val_accuracies):.2f}% (Epoch {val_accuracies.index(max(val_accuracies))+1})\")\n",
        "print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\n",
        "print(f\"Final test accuracy: {test_accuracy:.2f}%\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5WGiwGnK2lb",
        "outputId": "1bbdc2d3-8e1a-4baa-de4e-fd1edf290011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: saved_models/maryna_cnn_model.pth\n",
            "\n",
            "==================================================\n",
            "MODEL LOADING INSTRUCTIONS:\n",
            "==================================================\n",
            "To load the model:\n",
            "checkpoint = torch.load('saved_models/maryna_cnn_model.pth')\n",
            "model.load_state_dict(checkpoint['model_state_dict'])\n",
            "test_accuracy = checkpoint['test_accuracy']\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Save the final trained model\n",
        "import os\n",
        "\n",
        "# Create directory for saving models\n",
        "save_dir = \"saved_models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save the complete final model\n",
        "model_path = f\"{save_dir}/maryna_cnn_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'final_train_accuracy': train_accuracies[-1],\n",
        "    'final_val_accuracy': val_accuracies[-1],\n",
        "    'test_accuracy': test_accuracy,\n",
        "    'num_epochs': num_epochs,\n",
        "    'model_architecture': str(model),\n",
        "    'class_names': class_names,\n",
        "    'training_history': {\n",
        "        'train_losses': train_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accuracies': val_accuracies\n",
        "    }\n",
        "}, model_path)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL LOADING INSTRUCTIONS:\")\n",
        "print(\"=\"*50)\n",
        "print(\"To load the model:\")\n",
        "print(\"checkpoint = torch.load('saved_models/maryna_cnn_model.pth')\")\n",
        "print(\"model.load_state_dict(checkpoint['model_state_dict'])\")\n",
        "print(\"test_accuracy = checkpoint['test_accuracy']\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqgIeN3hLk2t"
      },
      "outputs": [],
      "source": [
        "# Code to load the saved model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Recreate the model architecture (must be identical to training)\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "\n",
        "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "\n",
        "    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.Linear(128*28*28, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.Linear(512, 2)\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the saved checkpoint\n",
        "checkpoint = torch.load('model_maryna.pth', map_location=device)\n",
        "\n",
        "# Load the model weights\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Access saved information\n",
        "test_accuracy = checkpoint['test_accuracy']\n",
        "class_names = checkpoint['class_names']\n",
        "training_history = checkpoint['training_history']\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "print(f\"Classes: {class_names}\")\n",
        "print(f\"Training completed in {checkpoint['num_epochs']} epochs\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
