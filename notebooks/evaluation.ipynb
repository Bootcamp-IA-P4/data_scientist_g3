{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZUdZUlLFYnb"
      },
      "source": [
        "# **Comparación de modelos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V73dPIqpFYnc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-S2HA90FYnd"
      },
      "outputs": [],
      "source": [
        "# Cargar el dataset de prueba\n",
        "X_test = pd.read_csv('../data/processed/preprocessing.csv')\n",
        "y_test = X_test.pop('stroke')  # Asumiendo que 'stroke' es la variable objetivo\n",
        "\n",
        "# Diccionario para almacenar las rutas de los modelos\n",
        "model_paths = {\n",
        "    'Gradient Boosting': '../models/MGB/ModelGB.pkl',\n",
        "    'LDA': '../models/lda/lda.pkl',\n",
        "    'LightGBM': '../models/ligthgbm/lightGBM_stroke_model.pkl',\n",
        "    'XGBoost': '../models/xgboost/xgboost_stroke_optimized_20250609_124848.pkl',\n",
        "    'Extra Trees': '../models/extra_trees_model.pkl'\n",
        "}\n",
        "\n",
        "# Diccionario para almacenar resultados\n",
        "results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByOIuSccFYnd",
        "outputId": "cafcdc56-253e-44b7-eb72-9c4b4b7bc384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error al evaluar LDA: The feature names should match those that were passed during fit.\n",
            "Feature names unseen at fit time:\n",
            "- smoking_status_Unknown\n",
            "- work_type_Govt_job\n",
            "- work_type_Never_worked\n",
            "\n",
            "Error al evaluar LightGBM: 'dict' object has no attribute 'predict'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [22:24:29] WARNING: /workspace/src/collective/../data/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
            "configuration generated by an older version of XGBoost, please export the model by calling\n",
            "`Booster.save_model` from that version first, then load it back in current version. See:\n",
            "\n",
            "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
            "\n",
            "for more details about differences between saving model and serializing.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ],
      "source": [
        " #Evaluar cada modelo\n",
        "for name, path in model_paths.items():\n",
        "    try:\n",
        "        # Cargar el modelo\n",
        "        model = joblib.load(path)\n",
        "\n",
        "        # Realizar predicciones\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Calcular métricas\n",
        "        precision_test = precision_score(y_test, y_pred)\n",
        "        recall_test = recall_score(y_test, y_pred)\n",
        "        f1_test = f1_score(y_test, y_pred)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        \n",
        "        results[name] = {\n",
        "            'Modelo': name,\n",
        "            'Precision Test': precision_test,\n",
        "            'Recall Test': recall_test,\n",
        "            'F1 Test': f1_test,\n",
        "            'ROC-AUC': roc_auc,\n",
        "            'Tipo de ajuste': 'Buen ajuste' if roc_auc > 0.8 else ('Underfitting' if roc_auc < 0.7 else 'Regular'),\n",
        "            'Umbral elegido': 0.5  # Umbral por defecto\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error al evaluar {name}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X6gXXegFYne"
      },
      "outputs": [],
      "source": [
        "# Crear DataFrame con las columnas especificadas\n",
        "cols = ['Ranking', 'Modelo', 'Precision Test', 'Recall Test', 'F1 Test', 'ROC-AUC', 'Tipo de ajuste', 'Umbral elegido']\n",
        "\n",
        "# Convertir resultados a DataFrame\n",
        "df_resumen = pd.DataFrame.from_dict(results, orient='index')\n",
        "\n",
        "# Calcular score compuesto basado en métricas de test\n",
        "df_resumen['Score Compuesto'] = (\n",
        "    df_resumen['F1 Test'] * 0.3 +        # F1 Score (30%)\n",
        "    df_resumen['ROC-AUC'] * 0.3 +        # ROC-AUC (30%)\n",
        "    df_resumen['Precision Test'] * 0.2 +  # Precision (20%)\n",
        "    df_resumen['Recall Test'] * 0.2       # Recall (20%)\n",
        ")\n",
        "\n",
        "# Agregar ranking basado en el score compuesto\n",
        "df_resumen['Ranking'] = df_resumen['Score Compuesto'].rank(ascending=False).astype(int)\n",
        "\n",
        "# Reordenar columnas\n",
        "cols.append('Score Compuesto')\n",
        "df_resumen = df_resumen[cols]\n",
        "\n",
        "# Agregar columnas interpretativas\n",
        "df_resumen['De 10 enviados, ¿cuántos realmente tienen ACV? (Precision)'] = (df_resumen['Precision Test'] * 10).round(1)\n",
        "df_resumen['De 10 enfermos reales, ¿cuántos detecta? (Recall)'] = (df_resumen['Recall Test'] * 10).round(1)\n",
        "df_resumen['¿Buen ajuste?'] = df_resumen['Tipo de ajuste'].apply(lambda x: 'Sí' if x == 'Buen ajuste' else 'No')\n",
        "\n",
        "# Mostrar resultados\n",
        "display(df_resumen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPtbWHTRFYne",
        "outputId": "9bb384c5-3683-4785-dadb-6982e3a5097d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Resultados de la comparación:\n",
            "                   accuracy  precision  recall      f1  roc_auc\n",
            "Gradient Boosting    0.8177     0.1693  0.6815  0.2713   0.8572\n",
            "XGBoost              0.7792     0.1592  0.8024  0.2657   0.8586\n",
            "\n",
            "Mejor modelo: Gradient Boosting\n"
          ]
        }
      ],
      "source": [
        "# Determinar el mejor modelo basado en F1 Test\n",
        "best_model_name = df_resumen.loc[df_resumen['Ranking'] == 1, 'Modelo'].iloc[0]\n",
        "best_model_path = model_paths[best_model_name]\n",
        "\n",
        "# Cargar el mejor modelo\n",
        "best_model = joblib.load(best_model_path)\n",
        "\n",
        "# Función para extraer y generar el pipeline\n",
        "def extract_pipeline(model):\n",
        "    steps = []\n",
        "\n",
        "    # Si el modelo ya es un pipeline, extraer sus pasos\n",
        "    if isinstance(model, Pipeline):\n",
        "        return model\n",
        "\n",
        "    # Si no es un pipeline, crear uno nuevo con el modelo\n",
        "    if hasattr(model, 'preprocessor_') and hasattr(model, 'estimator_'):\n",
        "        # Para modelos que tienen preprocessor y estimator separados\n",
        "        steps.extend([\n",
        "            ('preprocessor', model.preprocessor_),\n",
        "            ('estimator', model.estimator_)\n",
        "        ])\n",
        "    else:\n",
        "        # Para modelos simples\n",
        "        steps.append(('estimator', model))\n",
        "\n",
        "    return Pipeline(steps)\n",
        "\n",
        "    # Extraer/generar el pipeline del mejor modelo\n",
        "best_pipeline = extract_pipeline('best_model')\n",
        "\n",
        "# Crear el nombre del archivo para el pipeline\n",
        "pipeline_filename = f'best_pipeline_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
        "\n",
        "# Define the path to the models directory\n",
        "models_dir = 'models'\n",
        "# Create the models directory if it doesn't exist\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "\n",
        "pipeline_path = os.path.join(models_dir, pipeline_filename)\n",
        "\n",
        "# Guardar el pipeline\n",
        "joblib.dump(best_pipeline, pipeline_path)\n",
        "\n",
        "# Mostrar información del pipeline\n",
        "print(\"\\nPipeline del mejor modelo:\")\n",
        "for step_name, step in best_pipeline.steps:\n",
        "    print(f\"- {step_name}: {type(step).__name__}\")\n",
        "\n",
        "print(f\"\\nPipeline guardado en: {pipeline_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
